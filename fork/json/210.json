{"text": "---\nid: 210\nstate: approved\ncreated: 2018-08-20\nplacement:\n  category: design-patterns\n  order: 110\n---\n\n# Unicode\n\nAPIs should be consistent on how they explain, limit, and bill for string\nvalues and their encodings. This ranges from little ambiguities (like fields\n\"limited to 1024 characters\") all the way to billing confusion (are names and\nvalues of properties in Datastore billed based on characters or bytes?).\n\nIn general, if we talk about limits measured in bytes, we are discriminating\nagainst non-ASCII text since it takes up more space. On the other hand, if we\ntalk about \"characters\", we are ambiguous about whether those are Unicode \"code\npoints\", \"code units\" for a particular encoding (e.g. UTF-8 or UTF-16),\n\"graphemes\", or \"grapheme clusters\".\n\n## Unicode primer\n\nCharacter encoding tends to be an area we often gloss over, so a quick primer:\n\n- Strings are just bytes that represent numbers according to some encoding\n  format.\n- When we talk about **characters**, we sometimes mean Unicode **code points**,\n  which are numbers in the Unicode spec (up to 21 bits).\n- Other times we might mean **graphemes** or **grapheme clusters**, which may\n  have multiple numeric representations and may be represented by more than one\n  code point. For example, `\u00e1` may be represented as a composition of\n  `U+0061 + U+0301` (the `a` + the accent combining mark) or as a single code\n  point, `U+00E1`.\n- Protocol buffers uses **UTF-8** (\"Unicode Transformation Format\") which is a\n  variable-length encoding scheme using up to 4 **code units** (8-bit bytes)\n  per code point.\n\n## Guidance\n\n### Character definition\n\n**TL;DR:** In our APIs, \"characters\" means \"Unicode code points\".\n\nIn API documentation (e.g., API reference documents, blog posts, marketing\ndocumentation, billing explanations, etc), \"character\" **must** be defined as a\nUnicode code point.\n\n### Length units\n\n**TL;DR:** Set size limits in \"characters\" (as defined above).\n\nAll string field length limits defined in API comments **must** be measured and\nenforced in characters as defined above. This means that there is an underlying\nmaximum limit of (`4 * characters`) bytes, though this limit will only be hit\nwhen using exclusively characters that consist of 4 UTF-8 code units (32 bits).\n\nIf you use a database system (e.g. Spanner) which allows you to define a limit\nin characters, it is safe to assume that this byte-defined requirement is\nhandled by the underlying storage system.\n\n### Billing units\n\nAPIs **may** use either code points or bytes (using the UTF-8 encoding) as the\nunit for billing or quota measurement (e.g., Cloud Translation chooses to use\ncharacters). If an API does not define this, the assumption is that the unit of\nbilling is characters (e.g., $0.01 _per character_, not $0.01 _per byte_).\n\n### Unique identifiers\n\n**TL;DR:** Unique identifiers **should** limit to ASCII, generally only\nletters, numbers, hyphens, and underscores, and **should not** start with a\nnumber.\n\nStrings used as unique identifiers **should** limit inputs to ASCII characters,\ntypically letters, numbers, hyphens, and underscores\n(`[a-zA-Z][a-zA-Z0-9_-]*`). This ensures that there are never accidental\ncollisions due to normalization. If an API decides to allow all valid Unicode\ncharacters in unique identifiers, the API **must** reject any inputs that are\nnot in Normalization Form C. Generally, unique identifiers **should not** start\nwith a number as that prefix is reserved for Google-generated identifiers and\ngives us an easy way to check whether we generated a unique numeric ID for or\nwhether the ID was chosen by a user.\n\nUnique identifiers **should** use a maximum length of 64 characters, though\nthis limit may be expanded as necessary. 64 characters should be sufficient for\nmost purposes as even UUIDs only require 36 characters.\n\n**Note:** See AIP-122 for recommendations about resource ID segments.\n\n### Normalization\n\n**TL;DR:** Unicode values **should** be stored in [Normalization Form C][].\n\nValues **should** always be normalized into Normalization Form C. Unique\nidentifiers **must** always be stored in Normalization Form C (see the next\nsection).\n\nImagine we're dealing with Spanish input \"estar<b>\u00e9</b>\" (the accented part\nwill be bolded throughout). This text has what we might visualize as 6\n\"characters\" (in this case, they are grapheme clusters). It has two possible\nUnicode representations:\n\n- Using 6 code points: `U+0065` `U+0073` `U+0074` `U+0061` `U+0072`\n  **`U+00E9`**\n- Using 7 code points: `U+0065` `U+0073` `U+0074` `U+0061` `U+0072` **`U+0065`\n  `U+0301`**\n\nFurther, when encoding to UTF-8, these code points have two different\nserialized representations:\n\n- Using 7 code-units (7 bytes): `0x65` `0x73` `0x74` `0x61` `0x72` **`0xC3`\n  `0xA9`**\n- Using 8 code-units (8 bytes): `0x65` `0x73` `0x74` `0x61` `0x72` **`0x65`\n  `0xCC` `0x81`**\n\nTo avoid this discrepancy in size (both code units and code points), use\n[Normalization Form C][] which provides a canonical representation for strings.\n\n[normalization form c]: https://unicode.org/reports/tr15/\n\n### Uniqueness\n\n**TL;DR:** Unicode values **must** be normalized to [Normalization Form C][]\nbefore checking uniqueness.\n\nFor the purposes of unique identification (e.g., `name`, `id`, or `parent`),\nthe value **must** be normalized into [Normalization Form C][] (which happens\nto be the most compact). Otherwise we may have what is essentially \"the same\nstring\" used to identify two entirely different resources.\n\nIn our example above, there are two ways of representing what is essentially\nthe same text. This raises the question about whether the two representations\nshould be treated as equivalent or not. In other words, if someone were to use\nboth of those byte sequences in a string field that acts as a unique\nidentifier, would it violate a uniqueness constraint?\n\nThe W3C recommends using Normalization Form C for all content moving across the\ninternet. It is the most compact normalized form on Unicode text, and avoids\nmost interoperability problems. If we were to treat two Unicode byte sequences\nas different when they have the same representation in NFC, we'd be required to\nreply to possible \"Get\" requests with content that is **not** in normalized\nform. Since that is definitely unacceptable, we **must** treat the two as\nidentical by transforming any incoming string data into Normalized Form C or\nrejecting identifiers not in the normalized form.\n\nThere is some debate about whether we should view strings as sequences of code\npoints represented as bytes (leading to uniqueness determined based on the\nbyte-representation of said string) or to interpret strings as a higher level\nabstraction having many different possible byte-representations. The stance\ntaken here is that we already have a field type for handling that: `bytes`.\nFields of type `string` already express an opinion of the validity of an input\n(it must be valid UTF-8). As a result, treating two inputs that have identical\nnormalized forms as different due to their underlying byte representation seems\nto go against the original intent of the `string` type. This distinction\ntypically doesn't matter for strings that are opaque to our services (e.g.,\n`description` or `display_name`), however when we rely on strings to uniquely\nidentify resources, we are forced to take a stance.\n\nPut differently, our goal is to allow someone with text in any encoding (ASCII,\nUTF-16, UTF-32, etc) to interact with our APIs without a lot of \"gotchas\".\n\n## References\n\n- [Unicode normalization forms](https://unicode.org/reports/tr15/)\n- [Datastore pricing \"name and value of each property\"](https://cloud.google.com/datastore/pricing)\n  doesn't clarify this.\n- [Natural Language pricing](https://cloud.google.com/natural-language/pricing)\n  uses charges based on UTF-8 code points rather than code units.\n- [Text matching and normalization](https://sites.google.com/a/google.com/intl-eng/apis/matching?pli=1)\n"}